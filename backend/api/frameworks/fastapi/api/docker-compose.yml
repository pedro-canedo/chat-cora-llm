version: "3.3"

services:
  ollama:
    image: ollama/ollama
    volumes:
      - ollama:/root/.ollama
    ports:
      - "11434:11434"
    entrypoint: ["/bin/sh", "-c", "ollama run llama3 && tail -f /dev/null"]

  fastapi:
    build: .
    ports:
      - "8000:8000"
    environment:
      - MODEL_URL=http://ollama:11434/api/generate
    depends_on:
      - ollama

  starter_model:
    build: .
    command: ["./scripts/pull_llama3_model.sh"]
    depends_on:
      - ollama

  #mata o starter apos ele baixar o modelo
  # killer:
  #   build: .
  #   command: ["./scripts/kill_starter.sh"]
  #   depends_on:
  #     - starter_model

volumes:
  ollama:
